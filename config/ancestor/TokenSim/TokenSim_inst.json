{
    "model_name": "TokenSim",
    "tokenizer_name": "Unigram",
    "vocab_file": "data/ancestors/vocab",
    "train_file": "data/ancestors/train.txt",
    "dev_file": "data/ancestors/dev.txt",
    "test_file": "data/ancestors/test.txt",
    "inst_tokenizer_name": "Unigram",
    "include_city": "False",
    "include_state": "False",
    "include_country": "False",
    "include_type": "False",
    "random_seed": 2524,
    "codec": "utf-8" 
}
