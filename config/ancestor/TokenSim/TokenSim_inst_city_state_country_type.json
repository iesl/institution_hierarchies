{
    "model_name": "TokenSim",
    "tokenizer_name": "Unigram",
    "vocab_file": "data/ancestors/vocab",
    "train_file": "data/ancestors/train.txt",
    "dev_file": "data/ancestors/dev.txt",
    "test_file": "data/ancestors/test.txt",
    "inst_tokenizer_name": "Unigram",
    "city_tokenizer_name": "Unigram",
    "state_tokenizer_name": "Char",
    "country_tokenizer_name": "Char",
    "type_tokenizer_name": "Char",
    "include_city": "True",
    "include_state": "True",
    "include_country": "True",
    "include_type": "True",
    "random_seed": 2524,
    "codec": "utf-8" 
}
